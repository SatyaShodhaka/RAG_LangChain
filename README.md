# RAG Pipeline using LangChain

This project demonstrates a simple **Retrieval-Augmented Generation (RAG)** pipeline using the LangChain framework. It implements two main functionalities:

1. **Creating a vector store database (`create_db.py`)**
   - Load and preprocess markdown documents.
   - Split the documents into smaller chunks.
   - Store the document embeddings in a Chroma database.

2. **Querying the vector store database (`query_data.py`)**
   - Search for the most relevant document chunks based on a user query.
   - Use OpenAI’s API to generate answers based on the retrieved context.

## Features
- **Document Preprocessing**: Splits documents into smaller chunks for efficient retrieval.
- **Chroma Vector Store**: Stores document embeddings for fast similarity searches.
- **OpenAI API Integration**: Leverages OpenAI’s GPT model to generate responses using the retrieved context.
- **Command Line Interface (CLI)**: Enables easy interaction with the RAG pipeline through terminal commands.

---

## Installation and Setup

### Prerequisites
- Python 3.8 or higher
- `pip` for package management
- OpenAI API Key

### Environment Setup
1. Clone the repository:
    ```bash
    git clone https://github.com/SatyaShodhaka/RAG_LangChain/tree/main
    cd RAG_LangChain
    ```
2. Create and activate a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```
3. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```
4. Set up your `.env` file with your OpenAI API key:
    ```env
    OPENAI_API_KEY=<your_openai_api_key>
    ```

---

## Scripts Overview

### 1. `create_db.py`
This script creates a vector store database by processing and embedding the documents.

#### Key Functions:
- **`load_documents()`**: Loads `.md` files from the `data/` directory using LangChain’s `DirectoryLoader`.
- **`split_text(documents)`**: Splits the documents into smaller chunks using `RecursiveCharacterTextSplitter`.
- **`save_to_chroma(chunks)`**: Stores the document embeddings in a Chroma vector database.

#### Usage:
Run the script to create the database:
```bash
python create_db.py
```

#### Outputs:
- A `chroma/` directory containing the vector database.

### 2. `query_data.py`
This script allows you to query the vector database and generate answers based on the retrieved context.

#### Key Functions:
- **`similarity_search_with_relevance_scores(query_text)`**: Retrieves the top 3 most relevant chunks based on similarity scores.
- **`ChatPromptTemplate`**: Generates a prompt to pass to OpenAI’s GPT model using the retrieved context.
- **`ChatOpenAI`**: Sends the prompt to OpenAI’s API to generate a response.

#### Usage:
Run the script with a query as an argument:
```bash
python query_data.py "<your query here>"
```

#### Example:
```bash
python query_data.py "What is Retrieval-Augmented Generation?"
```

#### Outputs:
- A response generated by OpenAI based on the retrieved context.
- List of document sources used for generating the response.

---

## File Structure
```
.
├── chroma/                 # Vector store database (created by create_db.py)
├── data/                   # Directory containing markdown files
├── create_db.py            # Script to preprocess data and create vector store
├── query_data.py           # Script to query the vector store and generate answers
├── requirements.txt        # List of Python dependencies
├── .env                    # Environment variables (e.g., OpenAI API Key)
└── README.md               # Project documentation
```

---

## Example Workflow
1. Place your `.md` files in the `data/` directory.
2. Run the `create_db.py` script to preprocess and store the documents:
    ```bash
    python create_db.py
    ```
3. Query the database using `query_data.py`:
    ```bash
    python query_data.py "What is the purpose of the RAG pipeline?"
    ```

Sample Output:
```
API Key: sk-...
Split 5 documents into 30 chunks.
Saved 30 chunks to chroma.

Answer the question based only on the following context:

<retrieved context>

---

Answer the question based on the above context: What is the purpose of the RAG pipeline?

Response: The RAG pipeline combines retrieval and generation to generate answers based on a specific context from a knowledge base.
Sources: ['doc1.md', 'doc2.md']
```

---

## Notes
- Ensure the `data/` directory contains markdown files (`.md`) for processing.
- Use a secure method to manage your OpenAI API key.
- Adjust the `chunk_size` and `chunk_overlap` parameters in `create_db.py` for optimal performance based on your dataset.

---

## Dependencies
- `langchain`
- `langchain_community`
- `nltk`
- `openai`
- `python-dotenv`

Install all dependencies using:
```bash
pip install -r requirements.txt
```

---

## Troubleshooting
- **`Data path does not exist` error**: Ensure the `data/` directory exists and contains markdown files.
- **Low relevance scores in query results**: Verify the quality and size of the document chunks. Larger or more diverse datasets improve search results.
- **`OpenAI API Key` errors**: Ensure your `.env` file is correctly set up.

---

## Future Improvements
- Add support for additional file formats (e.g., PDF, TXT).
- Implement user authentication for secure API key usage.
- Optimize query performance for larger datasets.
- Explore alternative vector stores like FAISS or Pinecone.

---

## License
This project is licensed under the MIT License. Feel free to use, modify, and distribute the code.

---

## Author
[Your Name]  
[Your Contact Information]  
[Project Repository URL]

